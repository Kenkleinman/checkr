---
title: "Checking Code Answers"
author: "Daniel Kaplan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r include = FALSE}
library(ggplot2)
library(dplyr)
library(checkr)
library(magrittr)
```

> IN DRAFT: To do:    
>    * Add examples for `magrittr` pipelines.
>    * document the agrees() test-building function
>    * document `set_success_message()`
>    * document pre-evaluation testing.

To help meet the demand for R-related training, various *interactive tutorial systems* have been developed. These include [Swirl](http://swirlstats.com/), [DataComp.com](http://DataCamp.com), and the Shiny-based system introduced by RStudio: `[tutor](http://github.com/rstudio/tutor)`. In each of these systems, a problem is posed to the student, who has an opportunity to construct R commands as a solution. The R commands are then passed to an evaluation system.

The `checkr` package provides such an evaluation system that provides both evaluative and formative feedback and, at the instructor's discretion, logs student submissions for assigning credit and to support looking for common patterns of misconception in student answers.

"Evaluative feedback," in the jargon of education, means telling whether the student was right or wrong; whether the submitted work satisfied the requirements. In computer programming, evaluative feedback can be based on whether a correct result was computed, whether a function works as required, and so on. `Checkr` provides simple mechanisms for comparing the quantities computed by the student submission to known values.

"Formative feedback" attempts to help the student develop a better submission. This is a more subtle situation, since good feedback must involve understanding of how student's think about the problem, typical patterns in student answers, and the variety of ways that the problem can be approached. `Checkr` provides facilities for looking within the student's code for functions and their arguments, for checking intermediate results, and to look for specific mistake patterns.

Readers may also want to check out the system for *[submission correctness tests](https://www.datacamp.com/community/blog/submission-correctness-testing-in-datacamp-datacamp-part-i#gs.vuTG8Xw)* provided by DataCamp.


## A trivial example: 2 + 2

As an illustration of the need for an evaluation system and how `checkr` statements are writen, consider this trivial problem: 

> Write the code to add two and two. 

Of course, we expect the student to submit `2 + 2` as the response. But they might have

* submitted nothing
* submitted something like `3 + 1`
* submitted something like `2 / 2`

`Checkr` allows you to write test statements that

1. allow the values produced by the code to be tested against a specified solution and 
2. examine the student's submission to make sure the code was of the specified form.

A tutorial system will typically collect the student code submissions as text, and may evaluate the code in order to verify that it runs.  Here are a few different possible submissions for the `2 + 2` problem:

```r
2 + 2     # correct
2 / 2     # right arguments but wrong function
3 + 1     # right function but wrong arguments
`+`(2, 2) # correct
(2 + 2)   # correct
```

Note that it's not sufficient merely to check that the result of the calculation is the value 4. Submission three produces a value of 4 but is an incorrect answer to the stated problem. So in addition to checking the result, aspects of the way the result was calculated should also be checked. Here, that means perhaps these three things:

1. That `+` was used appropriately.
2. That the first argument to `+` is 2.
3. That is second argument to `+` is 2. 

Here are some tests from `checkr` that are relevant. Each test takes the form of a pattern and a message to return if that pattern is not found.
```{r}
library(checkr)
test_1 <- find_call("whatever + whatever", "you need to use addition (+)")
test_2 <- find_call("2 + whatever", "the first argument should be 2")
test_3 <- find_call("whatever + 2", "the second argument should be 2")
test_4 <- check_value(agrees(x == 4), "the result should be 4")
```

The first test makes sure that `+` has been used with two arguments, but disregards the values of the arguments. The second test checks that 2 is the first argument. The third test checks that 3 is the second argument. The messages for each test can be focussed as much as desired because the individual tests each look for a specific aspect of the calculation.


# Sequences of tests

The four tests given in the `2 + 2` example are not independent of each other. For instance, `test_2` will fail for a submission like `2 * 2`, but the resulting feedback message would not be helpful. To provide meaningful formative feedback, `test_2` has to follow `test_1`. 

`Checkr` tests can be formed into sequences, so that a failure of an early test prevents the triggering of later tests. The `find_` functions in `checkr` examine the submitted code for a pattern. If that pattern is not found, the test fails. The `find_call()` function knows about functions and arguments. The string `"whatever + whatever"` in `test_1` specifies that the function `+` should be called with two arguments, although the values of those arguments can be anything at all.

The `check_` functions in `checkr` test whether a output value or argument match specified conditions. For the `2 + 2` problem, an appropriate test sequence would be

```r
USER_CODE %>% test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4
```

The `final_` directive means to feed the last calculated value from the `USER_CODE` into the following test.

Ordinarily, these test statements would be used in conjunction with the interactive tutorial system. A later section of this document will how how to set up the "two plus two" problem using `tutor`. But first, let's examine how the tests perform on the several possible submissions listed above. Since the `USER_CODE` provided by `tutor`/`checkr` will not be available in this static document, I'll use `capture.code()` to mimic the capture and transmission of code from `tutor` to `checkr`.

```{r echo = FALSE}
print.capture <- function(test_output) {
  if (test_output$passed) cat("Passed!\n")
  else cat(paste("Sorry, but", test_output$message), "\n")
  return(invisible(test_output))
}
```

The obvious correct answer passes the tests.
```{r}
capture.code("2 + 2") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
```
Also passing are two variant correct forms:
```{r}
capture.code("`+`(2, 2)") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
capture.code("(2 + 2)") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
```

The incorrect submissions lead to the generation of a formative feedback message:

```{r}
capture.code("3 + 1") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
capture.code("2 / 2") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
```

# Using `checkr` with `tutor`

In a tutor document, the R/Markdown to set up the "two plus two" problem would look like this:

    1. Write a statement to calculate two plus two.
    
    ```{r, example-0, exercise = TRUE}
    # your code goes here
    ```

To tell `tutor` to use the `checkr` tests, the `setup` chunk at the beginning of the Rmd file should set the `exercise.checker` chunk option to be the `run_tests()` function from the `checkr` package.

    ```{r setup, include=FALSE}
    library(tutor)
    library(checkr)
    knitr::opts_chunk$set(exercise.checker = checkr::checkr_tutor)
    ```

When the student presses the "submit" button in the `tutor` code window, the student's code is sent off for checking.  You place the test statements in the problem's `-check` chunk, like this:

    ``{r example-0-check, echo=FALSE}
    test_1 <- find_call("whatever + whatever",  
                message = "need to use addition (+)")
    test_2 <- find_call("2 + whatever", 
                message = "first argument should be 2")
    test_3 <- find_call("whatever + 2", 
                message = "second argument should be 2")
    test_4 <- check_value(agrees(x == 4), 
                message = "the result should be 4")
    USER_CODE %>% test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4
    ```

The `tutor` system will call `checkr_tutor()` with information about the various chunks relating to the problem. In turn, `run_tests()` will evaluate the code in the `-check` chunk. The `USER_CODE` object is created by `run_tests()` to provide access to the code submitted by the user.


## Matching values

The R statements in the student's submission, presumably, produce values. These might be assigned a name with `<-` or they might be unnamed. Each complete statement produces a value.

The tests shown above for the "two plus two" problem don't actually require that the value produced be 4. For example, the following submission, `2 + 2 + 2` does not produce a value of 4. Nonetheless, it passes the tests:
```{r}
capture.code("2 + 2 + 2") %>% 
  test_1 %>% test_2 %>% test_3 
```
It's tempting to apply a test that says, "and nothing more."  I've rejected this strategy for the present, noting that sometimes a valid statement (e.g. `I(2 + 2)`) will have more than the test writer expected. Instead, you can test the values of the results produced by the student code.

```{r}
# these values are provided by tutor
USER_CODE   <- capture.code("2 + 2 + 2")
SOLN_CODE   <- capture.code("2 + 2")

# the -check chunk contents would look like
soln_test(USER_CODE, SOLN_CODE,
             res = final_,
             same_num(res)) 
```
The `soln_test()` function from `checkr` compares the values produced by the student's code and the values produced by code provided by the test writer in a `-solution` chunk. `USER_CODE` and `SOLN_CODE` convey the contents of the submission and solution chunks, respectively. 

The next argument to `soln_test()` uses a function `final_()`, which looks for the *last* value produced by the student and by the solution code. In this example, there's only one value, but in general there may be multiple values produced by submitted code. Whatever that last value be, and regardless of whether the student's or solution code assigned a name to the value, the argument name will be assigned to that value for future reference.

The final argument to `soln_test()` refers to the name `ref` created in the previous argument. The `same_num()` function compares the `ref` value in the student and the solution code. 

Later, we'll show examples using `soln_test()` that can deal with more complicated structure in the student and solution code. For instance, you can have as many reference-named values and comparison tests as you like, but that's hardly needed in this simple example.


## Framework for testing

Before it can be tested, the student answer must run, even if the answer is not correct. Parsing errors and run-time errors are absorbed by the `tutor` system, so any code with such errors will not be handed over to `checkr` for evaluation.

`Checkr` examines code one command line at a time. A command line is a complete piece of R code that generates a value, such as would be given in the console and evaluated when "return" is pressed. Here are some examples of statements:

* `x <- 7`
* `library(ggplot2)`
* `lm(mpg ~ hp, data = mtcars)`

Note that assignment of a name to a value, e.g. `x <- 7`, is part of the statement. 

At present, `checkr` tests are designed around command lines like the above.
When `magrittr` pipes are used, all the commands piped together constitute one statement. `checkr` will break such chains into individual statements, each having the contents between successive pipes.

Note that when curly braces are included in code, a command line is more inclusive than many people would think at first glance. For instance, `for` loops constitute a single a command line. The following chunk has only one command line: 

```r
for (k in 1:length(vec)) { 
  x <- cos(vec[k])
  sum <- sum + x
}
```

The same is true for `if` statements, such as:

```r
if (x > 0) {
  y <- cos(x)
} else {
  z <- sin(x)
}
```

Function definition with `function` also involves a single statement:

```r
funnel <- function(x, width=3) {
  ifelse(abs(x) > width, abs(x), -width)
}
```

In the future, we anticipate providing `checkr` tests that work with such constructions.



## Solution-matching tests and location tests

There are two basic kinds of `checkr` tests: solution-matching tests and location tests.

1. In a **solution-matching test**, a comparison is made between a solution provided by the problem author and the student's submitted code. Solution-matching tests are contained within the `soln_test()` function. 
2. In a **location test**, there is no reference to the author's solution. Instead, the test looks for patterns within the student's code. Location tests start with `USER_CODE %>% [one or more tests]`.

A `-check` chunk is composed of one or more test: solution-matching tests can be freely intermixed with location tests.

## Location tests

Location tests specify a pattern that is expected to be in the student code. Each location test either fails or identifies a single command line in the code being examined that matches the specified pattern. Location tests look through the command lines in a student answer to find the match. There are several ways to specify patterns:

- a character string (or regex) matches 
- a specified function (and, optionally, specified arguments) is called
- assignment to a name is done by the statement.

Location tests are often specified as sequences of tests, written

    USER_CODE %>% test_1 %>% test_2 ... and so on

The tests are run from left to right. Any test failing causes the test to terminate. The failure message from the whole sequence will be the failure message from the test that failed. If a test passes, the next test in the sequence is applied, and so on. 

The location tests currently provided by `checkr` are:

* `find_assignment("var")` looks for a statement which involves assignment to the variable named in the argument (`"var"` in this example). It can also take a regex. 
* `find_statement()` looks for a character string or regex match to the body of a statement.
* `find_value(1:10)` looks for a match to the value produced by a statement. Note that the argument is unquoted.
* `similar_names("sqrt(abs(x))")` looks for a statement that has similar names (function names, variables) to those contained in the statement provided as an argument
* `final()` identifies the final statement. No argument necessary
* `find_call("cos(x + whatever)")` looks for a statement containing a function call that matches the one provided as an argument. 
* `find_names(sqrt(3 * x + y))` will look for a statement that contains function and object names matching those in the argument. Note that the argument is unquoted.
* `find_constants(3, "skippidee")` looks for a statement with numbers and character strings matching those given in the arguments. You can include as many constants as you like.
* `find_formula()` looks for a statement containing a formula.

Each passing test records the particular command line that matched the pattern. Subsequent tests can be directed to look for matches before, after, or within the command line that matched the previous test. See the location-qualifier functions \code{then}, \code{previously}, and \code{inside}. Of course, you need not use a location qualifier, in which case subsequent tests will look at *all* command lines.

There are location tests that allow you to test a value generated by the code. The simplest is \code{check_value}, which looks at the value of the whole command line. A previous locator test should have identified the particular line whose value is to be tested.  For example:

```{r}
USER_CODE <- capture.code("x <- 7 + 3\n sin(x)")
test_1 <- find_call("sin(whatever)", "you didn't use sin().")
test_2 <- check_value(match_number(sin(10), "something's wrong with the sin() line."))
USER_CODE %>%
  test_1 %>% test_2 
```
  
Another kind of value-checking location test looks for the value of an argument to a function. The location specifier must include a \code{grab_this} to identify the particular argument to be captured. For instance:

```{r}
test_a <- find_call("whatever + whatever", "remember to use +") # regular location test
test_b <- check_argument("grab_this + whatever", 
                         match_number(17, tol = 0.1))
USER_CODE %>%
  test_a %>% test_b
```

## Combining tests

Combining two tests means to use the output of one test as the intput to another. Here are functions that combine tests in various ways. All of them produce a function as output. This output function takes a capture object as input to carry out the test.


* `any_test(t1, t2, ...)` checks whether any of the tests pass. If so, the overall result is a pass. Stops when it first finds a passing match.
* `all_tests(t1, t2, ...)` requires all the tests to pass. This accomplishes the same thing as combining the tests with `%>%`. Stops when it first encounters a failing match. 
* `branch_test(condition, yes_test, no_test)` evaluate the condition test on the capture object input. If it passes, then the result of the overall test will be the result of running the yes-test on the capture object input. Otherwise, the result of the no-test on the capture object will be the output.

## Common student mistakes

If you know about a common sort of student mistake, you may want to test specifically for that in order to give a helpful failure message. To do this, construct a test for the anticipated mistake, following it with `was_mistake()`. For example:

> What is the $x$-coordinate of a point on the unit circle at $\pi/2$?

```{r}
USER_CODE <- capture.code("sin(pi / 2)") 
test_1 <- find_call("pi/2", "you need to compute pi / 2.")
test_2 <- find_call("sin()") # wrong, but common mistake
test_3 <- find_call("cos()")
USER_CODE %>% 
  test_1 %>% test_2 %>% 
  was_mistake(message = "the x-coordinate is given by the cosine function") %>% 
  test_3 %>% 
  final_ %>% check_value(match_number(cos(pi/2)))() 
```
## Example: Testing assignment

Consider a problem like this:

> Assign the name `x` to the results of the calculation $\sqrt{7}$.

There are several things to be checked here:

1. That a name was assigned to an object.
2. That the name was `x`
3. That `sqrt()` was called.
4. That 7 was the argument to `sqrt()`.

It's the author's choice whether to test all these things. Here are some locator test sequences that an author might choose among.

```{r}
USER_CODE <- capture.code("xx <- sin(7)") # wrong in so many ways!
test_1 <- find_call("sqrt(whatever)", "use the sqrt() function.")
test_2 <- find_assignment("x")
test_3 <- check_argument("sqrt(grab_this)", match_number(7))
```

Which of these tests should come first? It depends on the points the author wants to emphasize. For example, she might want to know first whether assignment  was used.
```{r}
USER_CODE %>% test_2 %>% test_1 %>% test_3 
```
Or, the author might want to check first that the `sqrt()` function was used.
```{r}
USER_CODE %>% test_1 %>% test_3 %>% test_2 
```


## Creating and debugging tests

Note that in the examples, the location tests are all created and named *before* being used in the `USER_CODE %>% ...` checking line. Each test is implemented as an unevaluated function.

You might **incorrectly** write
```r
USER_CODE %>% find_call("whatever + whatever", "remember to use +")
```
This is not a check statement. Instead, the statement passes `USER_CODE` to the `find_call()` function. But `find_call()` doesn't take user code as an input. Rather, `find_call()`'s first argument should be the pattern to be matched. The output of `find_call()` is another function. That function *does* take user code as input.

In debugging a single test such as `test_a`, you should call `debug(test_a)`. This "functional programming" style has the potential to be confusing to many users.

## A richer example

Here's a problem that's more difficult: Using `ggplot()`, make a scatterplot of miles-per-gallon versus horsepower in the `mtcars` data.

Let's look at these possible submissions:

```{r}
submission_1 <- "ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point()" # Wrong!
submission_2 <- "ggplot(mtcars, aes(x = hp, y = mpg)) + geom_line()" # Wrong!
submission_3 <- "ggplot(mtcars, aes(y = mpg, x = hp)) + geom_point()" # right
submission_4 <- "ggplot(mtcars) + geom_point(aes(y = mpg, x = hp))" # also right
submission_5 <- c(
"my_cars <- mtcars",
"ggplot(my_cars, aes(y = mpg, x = hp)) + geom_point()")
submission_6 <- c(
"my_cars <- mtcars %>% select(mpg, hp)",
"ggplot(my_cars, aes(y = mpg, x = hp)) + geom_point()")
```

Submission 1 has the roles of the variables `mpg` and `hp` reversed from the problem statement. Submission 2 uses `geom_line()` instead of the required `geom_point()`. Submissions 3 and 4 will all create the specified plot, but each is arranged in a different way.

Here are some tests. Remember, the second argument is the message to return if the test *fails*.
```{r}
library(ggplot2)
test_1 <- find_call("aes(x = hp, y = whatever)", "variable 'hp' goes on the x axis")
test_2 <- find_call("aes(y = mpg, x = whatever)", "variable 'mpg' goes on the y axis")
test_3 <- find_call("geom_point()", "include a 'geom_point()' layer")
test_4 <- find_statement("mtcars") 
test_5 <- find_call("ggplot(data = whatever)", "no data handed to ggplot()")
test_6 <- check_argument("ggplot(data = grab_this)", test = match_class("data.frame"))
test_7 <- check_argument("ggplot(data = grab_this)", match_data_frame(mtcars))
test_8 <- check_argument("ggplot(data = grab_this)", 
                         match_data_frame(mtcars, hint = TRUE))
test_9 <- check_argument("ggplot(data = grab_this)",
                         match_data_frame(
                           mtcars %>% select(hp, mpg, carb), 
                           hint = TRUE))
```

We'll chain these seven tests together and see the results for the different submissions.

```{r}
capture.code(submission_1) %>% 
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
capture.code(submission_2) %>%
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
capture.code(submission_3) %>% 
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
capture.code(submission_4) %>%
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
```

These examples show passes and fails when checking the value of an argument to a function.

```{r}
capture.code(submission_5) %>% test_5 %>% test_7 
capture.code(submission_6) %>% test_5 %>% test_7 
capture.code(submission_6) %>% test_5 %>% test_8 
capture.code(submission_6) %>% test_5 %>% test_9 
```
  

## Checking values

Each command produces a value. You have access to these, both named and unnamed. 

For instance, this command does some arithmetic and creates a `ggplot` object.

```{r}
submission_1 <- "
3 + 5
ggplot(mtcars, aes(y = mpg, x = hp)) + geom_point()" 
```

A simple test for this is:
```{r}
test_1 <- find_value(match_class("ggplot"))
capture.code(submission_1) %>% test_1
```

But if we were looking for a `lattice` graphics object, the submission fails:
```{r eval = FALSE}
test_2 <- find_value(match_class("lattice"))
capture.code(submission_1) %>% test_2 
```

If either would suffice ...
```{r}
test_3 <- any_test(test_1, test_2)
capture.code(submission_1) %>% test_3 
```

Suppose that we want to check that a number near 8 is created ...
```{r}
test_4 <- find_value(match_number(8, range = c(7.9, 8.1)))
capture.code(submission_1) %>% test_4 
```

## Checking the value itself


A sequence of odd numbers from 11 to 31 (inclusive) using the colon operator.

```{r}
submission_1 <- capture.code("seq(11, 31, by = 2)") # right value, but not what was asked
submission_2 <- capture.code("11 + 2*(0:10)") # right
submission_3 <- capture.code("11 + 2*(1:11)") # uses colon, but wrong result

```

```{r}
test_1 <- find_call("whatever : whatever", "you didn't use the colon operator")
test_2 <- check_value(match_vector(seq(11, 31, by = 2), hint = TRUE))

submission_1 %>% test_1 %>% test_2 
submission_2 %>% test_1 %>% test_2 
submission_3 %>% test_1 %>% test_2 
```



Suppose we ask the student to construct a somewhat complicated object that we want to run some checks on. For instance, "Construct a model of `mpg` versus `hp` using `wt` as a covariate."

```{r}
submission_1 <- capture.code("lm(mpg ~ hp, data = mtcars)")  # wrong
submission_2 <- capture.code("lm(mpg ~ hp + wt, data = mtcars)") # right
submission_3 <- capture.code("lm(mpg ~ wt, data = mtcars)") # wrong
```

```{r}
test_1 <- find_call("lm(data = mtcars)", "use lm() on mtcars data")
test_2 <- check_value(function(x) {'wt' %in% names(coef(x))}, 
                      "what about the covariate wt?")
test_3 <- check_value(function(x) {all(c("hp", "wt") %in% names(coef(x)))}, 
                      'include both hp and the covariate as explanatory variables')
test_4 <- check_argument("lm(formula = grab_this)", match_formula(mpg ~ hp + wt))
```

```{r eval = FALSE}
submission_1 %>% test_1 %>% test_2 %>% test_3 
submission_2 %>% test_1 %>% test_2 %>% test_3
submission_3 %>% test_1 %>% test_2 %>% test_3 
submission_1 %>% test_1 %>% test_4 
submission_3 %>% test_1 %>% test_4 
```

## Comparing to a given solution

IN DRAFT. TALK ABOUT `closest_to("what")`, which finds the value in the user code that's closest to the value in the solution code identified by the <what> string. The <what> string *must* be a parsable command, but does not need to be the whole line of code. Alternatively, you can give `closest_to()` an unquoted name, corresponding to some named object produced by the solution code.

Often, perhaps most of the time, the question author will provide an example solution. The `soln_test()` function lets you compare the values generated by the solution to the values generated by the user's code.

To illustrate, let's construct by hand the `USER_CODE` and `SOLN_CODE` contents that get produced from the question chunks in a `tutor` document.
```{r}
USER_CODE <- capture.code("2 + 3")
SOLN_CODE <- capture.code("2 + 2")
soln_test(USER_CODE, SOLN_CODE,
             res = find_statement("2 *\\+", regex = TRUE),
             same_num(res)) 
```
The first two arguments to `soln_test()` are always the user code and the solution code in that order. The following arguments to `soln_test()` are either named or unnamed. 

Named arguments -- e.g. `res = instatements(regex="2 *\\+")` -- are often set to be locator statements. The name itself will be used to refer to the value for the command found by the locator statement. Alternatively, named arguments can be used to calculate some value from those already located and named.

Unnamed arguments -- e.g. `same_num(res)` -- are a comparison test selected from the `same_()` family of tests provided by `checkr`. 

In operation, `soln_test()` extracts out the values from the user code and the solution code for the command that passes the locator statement. The comparison tests are then run to compare the user values and the solution values.

You can compute elements from the named values. For instance, here we compare not the result itself but the absolute value of the result. Thus, the user code, which does not match the solution code, still passes the `same_num()` test, since the value $-4$ computed by the user passes the test.

```{r}
USER_CODE <- capture.code("2 + -6")
SOLN_CODE <- capture.code("2 + 2")
soln_test(USER_CODE, SOLN_CODE,
             res = find_statement("2 *\\+", regex = TRUE),
             same_num(abs(res))) 
```

The point of having such transformations -- e.g. `abs()` in the above example -- is to enable the test writer to compare complex objects like models using simple comparisons. For instance, suppose the user is asked to construct a model based on `mtcars`:
```{r eval = FALSE}
USER_CODE <- capture.code("mod <- lm(mpg ~ hp + carb, data = mtcars)")
SOLN_CODE <- capture.code("mod <- lm(mpg ~ hp * carb, data = mtcars)")
soln_test(USER_CODE, SOLN_CODE,
             res = find_assignment("mod"),
             same_vec(coef(res)))
```



## Magrittr pipelines

Magrittr pipelines are translated into a sequence of commands. For instance,
```
foobar <- mtcars %>% filter(mpg > 15) %>% select(mpg)
```
will be evaluated as if it were
```
filter(mtcars, mpg > 15) -> ..tmp1..
select(..tmp1.., mpg) -> foobar
```

This allows tests to be constructed as if each step in the chain were a separate command; the test statements can be constructed in the ordinary way. Do remember that the first argument to `dplyr` data verbs is `.data =`.


```{r eval = FALSE}
submission_1 <- capture.code("foobar <- mtcars %>% filter(mpg > 15)")
test_1 <- find_call("filter()", "should call filter()")
test_2 <- check_argument("mpg > grab_this", match_number(15))
test_2A <- check_argument("mpg > grab_this", match_number(16))
test_3 <- check_argument("filter(.data = grab_this)", match_data_frame(iris, hint = TRUE))
submission_1 %>% test_1 %>% test_2 %>% test_3 
submission_1 %>% test_1 %>% test_2A %>% test_3 

```

STILL NEED TO HANDLE INTERNAL CHAINS and EXPLICIT "dot" arguments.

This is not working. I think I need a new check_argument_symbol() that won't try to evaluate the symbols.
```{r}
submission_2 <- capture.code("mtcars %>% filter(mpg > 15) %>% group_by(cyl) %>% summarise(mmpg = mean(mpg))")
test_4 <- find_call("group_by()")
test_5 <- check_argument("group_by(.data = whatever, grab_this)", function(x) x)
submission_2 %>% test_4 %>% test_5
```

## Pre-checking user code

Before the locator and value tests can be applied to user code, that code must run successfully (even if it doesn't satisfy the instructions given in the exercise).

When the user code doesn't run, an R error will be displayed to the user. For many purposes, the tutorial author may regard this as perfectly satisfactory. But there may be circumstances where the author would like to intercept such errors in order to display a gentler or more helpful message.

By creating a chunk whose label has the extension `-code-check`, you can set up `checkr` tests that don't rely on the code being syntactically correct or otherwise not producing errors. For instance, for the `example-0` exercises, you can add a chunk like this:

    ```{r, example-0-code-check, exercise = TRUE}
    USER_CODE %>% check_assignment_names()
    USER_CODE %>% check_function_calls("sqrt") 
    ```

This particular set of instructions checks whether names used in assignment are formed legally and whether `sqrt` is used as a function (if it is used at all). Being "used as a function" simply means that there are open and closed parentheses. In the future, additional checks may be added.

Notice that the pre-evaluation checking uses a different set of functions than the locator and value tests. That's because locator and value tests work on the assumption that the code being given them will evaluate properly. 


## Logging and authentication

> NOTE: This system is under development. We need to settle on a way to have persistent storage on `shinyapps.io`. Right now, this will work only on a user's machine. It still needs to be tested even on a regular shiny server.

Each time an exercise submission is made, `checkr` logs information about the submission: user ID (if any), time stamp, user code, the message returned in response by `checkr`, and whether the submission was deemed correct. The log-file, `"log-test.txt"`, will eventually be made to live in the same directory as the Rmd file containing the tutorial. It is a flat, plain text file, with each submission recorded in JSON format as a list.

If you are using the tutorial as part of a course, you may wish to record the student's submissions for scoring. Or, if you are writing a tutorial, you might want that record in order to see what kind of code submissions students have made, so that you can write better tests for them.

If you want to record the user's ID along with the other submission data, add this chunk, verbatim, to the tutor Rmd document.

    ```{r child = checkr::authentication()}
    ```

This will display text-entry fields for the student to enter an ID and password.

The current version simply takes the entered ID at face value and checks whether the password is literally `pass`. Later versions will take the name of a file containing user-ID/password pairs.





